{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTGUq25AwGK7"
   },
   "source": [
    "# 1A. Mean, Median, Mode, Variance, Std, Norm, MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpOd99O5wYar",
    "outputId": "a4c0ce02-8f7f-4b0e-b415-f3dbe43bbf85"
   },
   "outputs": [],
   "source": [
    "\n",
    "import statistics as st\n",
    "import numpy as np\n",
    "import math\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "# Operations on 1-D array\n",
    "#arr1 = [10, 23, 3, 10, 121, 7, 10, 76, 42, 89]\n",
    "arr1 = [1,2,3,4,5,2,2,1,2,3]\n",
    "arr1.sort()\n",
    "n = len(arr1)\n",
    "\n",
    "# Mean\n",
    "mean = sum(arr1) / len(arr1)\n",
    "print(\"Mean: \", mean)\n",
    "\n",
    "# Median\n",
    "if(n%2 == 0):\n",
    "  median = (arr1[int(n/2)-1] + arr1[int(n/2)]) / 2\n",
    "else:\n",
    "  median = arr1[ int(n/2) ]\n",
    "print(\"\\nMedian: \", median)\n",
    "\n",
    "# Mode\n",
    "a = []\n",
    "i = 0\n",
    "while i < len(arr1):\n",
    "  a.append(arr1.count(arr1[i]))\n",
    "  i += 1\n",
    "d1 = dict(zip(arr1, a))\n",
    "mode = { k for (k,v) in d1.items() if v==max(a)}\n",
    "print(\"\\nMode:\", mode)\n",
    "\n",
    "# Variance and Standard deviation\n",
    "summ = 0\n",
    "for ele in arr1:\n",
    "  summ += pow((mean-ele),2)\n",
    "variance = summ/(len(arr1)-1)\n",
    "print(\"\\nVariance: \", variance)\n",
    "\n",
    "std = math.sqrt(variance)\n",
    "print(\"\\nStandard Deviation: \", std)\n",
    "\n",
    "# Mim-Max Normalisation\n",
    "mini = min(arr1)\n",
    "maxi = max(arr1)\n",
    "norm = []\n",
    "for i in arr1:\n",
    "  nr = (i-mini)/(maxi-mini)\n",
    "  norm.append(round(nr, 2))\n",
    "\n",
    "d1 = dict(zip(arr1, norm))\n",
    "print(\"\\nMin Max Normalisation --> \")\n",
    "print(d1)\n",
    "\n",
    "#Standardisation\n",
    "standardised = []\n",
    "for ele in arr1:\n",
    "  sd = (ele-mean)/std\n",
    "  standardised.append(round(sd, 2))\n",
    "\n",
    "d2 = dict(zip(arr1, standardised))\n",
    "print(\"\\nStandardisation --> \")\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxDRq_cC5xj8"
   },
   "source": [
    "# 2. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "-yiV1__754to",
    "outputId": "8f5baab1-5898-44b7-9693-5a79001665aa"
   },
   "outputs": [],
   "source": [
    "''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 3\n",
    "data = pd.read_csv('Lab 03 kmeans.csv')\n",
    "x1 = data['X1'].values\n",
    "x2 = data['X2'].values\n",
    "X = np.array(list(zip(x1, x2)))\n",
    "\n",
    "C_x = [6.2, 6.6 ,6.5]\n",
    "C_y = [3.2, 3.7, 3.0]\n",
    "centroid = np.array(list(zip(C_x, C_y)))\n",
    "centroid_old = np.zeros(centroid.shape)\n",
    "print(\"Initial Centroids: \\n\", centroid_old)\n",
    "\n",
    "def euclidean(a, b, ax=1):\n",
    "  return np.linalg.norm(a-b, axis=ax)\n",
    "\n",
    "clusters = np.zeros(len(X))\n",
    "error = euclidean(centroid, centroid_old, None)\n",
    "print(\"Initial Clusters\", clusters)\n",
    "print(\"Initial Error ........ \", error)\n",
    "iter = 0\n",
    "\n",
    "while error != 0:\n",
    "  iter = iter + 1\n",
    "  for i in range(len(X)):\n",
    "    distances = euclidean(X[i], centroid)\n",
    "    cluster = np.argmin(distances)\n",
    "    clusters[i] = cluster\n",
    "\n",
    "  centroid_old = deepcopy(centroid)\n",
    "  for p in range(k):\n",
    "    points = [X[j] for j in range(len(X)) if clusters[j] == p]\n",
    "    centroid[p] = np.mean(points, axis=0)\n",
    "\n",
    "  print(\"\\nCentroid after \", iter, \" iteration \\n\", centroid)\n",
    "  error = euclidean(centroid, centroid_old, None)\n",
    "  print(\"Error ....... \", error)\n",
    "\n",
    "  print(\"Clusters: \", clusters)\n",
    "\n",
    "# Clustring representation\n",
    "plt.scatter(x1, x2, c=clusters)\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OII01r3PNb4p"
   },
   "source": [
    "# 3. Binary Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JxvvHBESNh5j",
    "outputId": "8d58aac4-5726-45b9-e01a-a72cdbbdd09e"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "data = pd.read_csv('Lzoo_data.csv')\n",
    "data\n",
    "\n",
    "X = data.values[:, 0:16]\n",
    "y = data.values[:, 16]\n",
    "\n",
    "# Considering Entropy as Criteria\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "model1 = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred_1 = model1.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_1))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_test, y_pred_1))\n",
    "print(\"\\nClassification Report: \\n\", classification_report(y_test, y_pred_1))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_tree(model1)\n",
    "plt.show()\n",
    "\n",
    "# Considering Gini as Criteria\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "model2 = DecisionTreeClassifier(criterion=\"gini\")\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred_2 = model1.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_2))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_test, y_pred_2))\n",
    "print(\"\\nClassification Report: \\n\", classification_report(y_test, y_pred_2))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_tree(model2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFjeg6JsNgDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XH4riCdXtLu"
   },
   "source": [
    "# 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "8fu4Zm4LXvsV",
    "outputId": "5779d388-acb5-417e-fbb6-24073f93985a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "headers = ['X', 'Y']\n",
    "data = pd.read_csv('Lab 05 Food-Truck.csv', names=headers)\n",
    "X = data['X'].values\n",
    "y = data['Y'].values\n",
    "\n",
    "# Predicting\n",
    "mx = np.mean(X)\n",
    "my = np.mean(y)\n",
    "sx = st.stdev(X)\n",
    "sy = st.stdev(y)\n",
    "\n",
    "r = np.corrcoef(X, y)[0][1]\n",
    "m = r * (sy/sx)\n",
    "c = my - (m * mx)\n",
    "\n",
    "y_pred = (m * X) + c\n",
    "\n",
    "# Evaluating the predictions\n",
    "error = (y - y_pred)**2\n",
    "sse = sum(error)\n",
    "print(\"Sum Squared Error --> \", sse)\n",
    "\n",
    "mse = sse/len(y)\n",
    "print(\"\\nMean Squared Error --> \", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"\\nRoot Mean Squared Error --> \", rmse)\n",
    "\n",
    "er1 = (y - my)**2\n",
    "sst = sum(er1)      \n",
    "print(\"\\nSum of squares total --> \", sst)\n",
    "\n",
    "pred_mean = np.mean(y_pred)\n",
    "er2 = (y_pred - pred_mean)**2\n",
    "ssr = sum(er2)      \n",
    "print(\"\\nSum of squares regression --> \", ssr)\n",
    "\n",
    "r_score = ssr/sst\n",
    "print(\"\\nR2 Score --> \", r_score)\n",
    "\n",
    "print(\"\\n Plotting Graph of Actual vs Predicted \\n\")\n",
    "plt.scatter(X, y, color='g', s=30)\n",
    "plt.plot(X, y_pred, color='r')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7nefezQvHnc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVxPA77mBiMn"
   },
   "source": [
    "# 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NxykchANE4QH",
    "outputId": "43bd9ef4-52af-44e9-cd9d-bed7ca59f617"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "data = pd.read_csv('Student-University.csv')\n",
    "x = data.values[:, 0:2]\n",
    "y = data.values[:, 2]\n",
    "\n",
    "preX = preprocessing.scale(x)\n",
    "foldings = KFold(n_splits=5)\n",
    "\n",
    "for train_i, test_i in foldings.split(preX):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(preX, y, test_size=0.25)\n",
    "    epoch=10000\n",
    "    alpha=0.001\n",
    "    b0=0\n",
    "    b1=0\n",
    "    b2=0\n",
    "    x1 = X_train[:, 0]\n",
    "    x2 = X_train[:, 1]\n",
    "    while(epoch>0):\n",
    "        for i in range(len(X_train)):\n",
    "            pred = b0 + b1*x1[i] + b2*x2[i]\n",
    "            prediction = 1 /(1 + np.exp(-pred))\n",
    "            b0 = b0 + alpha*(y_train[i]-prediction)*prediction*(1-prediction)*1.0\n",
    "            b1 = b1 + alpha*(y_train[i]-prediction)*prediction*(1-prediction)*x1[i]\n",
    "            b2 = b2 + alpha*(y_train[i]-prediction)*prediction*(1-prediction)*x2[i]\n",
    "            epoch = epoch-1\n",
    "\n",
    "print(b0, b1, b2)\n",
    "\n",
    "x3 = X_test[:, 0]\n",
    "x4 = X_test[:, 1]\n",
    "ypred = [0]*len(X_test)\n",
    "ylist = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "  pred = b0 + b1*x3[i] + b2*x4[i]\n",
    "  ypred[i] = np.round(1 / (1 + np.exp(-pred)))\n",
    "  ylist.append(round(ypred[i]))\n",
    "\n",
    "print(y_test)\n",
    "print(ylist)\n",
    "\n",
    "score = accuracy_score(y_test, ypred)\n",
    "print(\"Accuracy: \", score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4LeF-IkG30Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3fHh0jNMKYc"
   },
   "source": [
    "# 6. Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q8tYPLUcMQUk",
    "outputId": "8193d675-816f-4ab4-eca5-49d71ac52218"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('Lab 07 iris.csv')\n",
    "\n",
    "X = data.values[:, 0:4]\n",
    "y = data.values[:, 4]\n",
    "\n",
    "print(X[:4, 0:3])\n",
    "\n",
    "scaleX = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Eigen Values and Vectors\n",
    "features = scaleX.T\n",
    "covMatrix = np.cov(features)\n",
    "values, vectors = np.linalg.eig(covMatrix)\n",
    "print(values)\n",
    "print(vectors)\n",
    "\n",
    "# Variance of each feature\n",
    "eachVar = []\n",
    "for i in range(len(values)):\n",
    "  res = values[i]/np.sum(values)*100\n",
    "  eachVar.append(res)\n",
    "\n",
    "print(eachVar)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(4), eachVar, alpha=0.8)\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Precentage of eachVar')\n",
    "plt.show()\n",
    "\n",
    "pc1 = scaleX.dot(vectors.T[0])\n",
    "pc2 = scaleX.dot(vectors.T[1])\n",
    "result = pd.DataFrame(pc1, columns=[\"PC1\"])\n",
    "result[\"PC2\"] = pc2\n",
    "result[\"Y\"] = y\n",
    "print(result.head(10))\n",
    "\n",
    "sns.FacetGrid(result, hue='Y', height=6).map(plt.scatter, 'PC1', 'PC2').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWDzfFIyUXSm"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eaxi7VqDUXuC"
   },
   "source": [
    "# 7. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUhX-2ugVnZq",
    "outputId": "735825e5-019b-4b7a-887b-7279f427468a"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "data = pd.read_csv('glass.csv')\n",
    "y = data['Type']\n",
    "x = data.drop('Type', 1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "linear = svm.SVC(kernel='linear')\n",
    "linear.fit(xtrain, ytrain)\n",
    "ypred1 = linear.predict(xtest)\n",
    "\n",
    "#print(linear.support_vectors_)\n",
    "#print(linear.n_support_)\n",
    "\n",
    "print(\"Accuracy Of Linear Kernel: \", accuracy_score(ytest, ypred1))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, ypred1))\n",
    "print(\"Classification report: \\n\", classification_report(ytest, ypred1))\n",
    "\n",
    "sigmoid = svm.SVC(kernel='sigmoid')\n",
    "poly = svm.SVC(kernel='poly')\n",
    "rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "sigmoid.fit(xtrain, ytrain)\n",
    "poly.fit(xtrain, ytrain)\n",
    "rbf.fit(xtrain, ytrain)\n",
    "\n",
    "ypred2 = sigmoid.predict(xtest)\n",
    "ypred3 = poly.predict(xtest)\n",
    "ypred4 = rbf.predict(xtest)\n",
    "\n",
    "print(\"Accuracy Of Sigmoid Kernel: \", accuracy_score(ytest, ypred2))\n",
    "print(\"Accuracy Of Ploy Kernel: \", accuracy_score(ytest, ypred3))\n",
    "print(\"Accuracy Of RBF Kernel: \", accuracy_score(ytest, ypred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk0VLi6CVHkK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNvwJf6oa5dT"
   },
   "source": [
    "# 8. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "_BPU3v3ra84J",
    "outputId": "450895b3-3396-4906-dbd0-92c20dbc957c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "data = pd.read_csv('Lab 09 covid.csv')\n",
    "data.head()\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "pc = le.fit_transform(data['pc'].values)\n",
    "wbc = le.fit_transform(data['wbc'].values)\n",
    "mc = le.fit_transform(data['mc'].values)\n",
    "ast = le.fit_transform(data['ast'].values)\n",
    "bc = le.fit_transform(data['bc'].values)\n",
    "ldh = le.fit_transform(data['ldh'].values)\n",
    "y = le.fit_transform(data['diagnosis'].values)\n",
    "\n",
    "X = np.array(list(zip(pc, wbc, mc, ast, bc, ldh)))\n",
    "#print(X)\n",
    "#print(y)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
    "naivee = MultinomialNB()\n",
    "naivee.fit(xtrain, ytrain)\n",
    "ypred = naivee.predict(xtest)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(ytest, ypred))\n",
    "print(\"Classification Report: \\n\", classification_report(ytest, ypred))\n",
    "\n",
    "yp = naivee.predict_proba(xtest);\n",
    "roc_curve(ytest, yp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwtx2ivfgSQR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7laua2X6gSav"
   },
   "source": [
    "# 9. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQb4iW3ugWq_",
    "outputId": "ccf4c98a-0f9e-4442-b42b-eea40dc828fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('pima.csv')\n",
    "y = data['Outcome']\n",
    "X = data.drop('Outcome', 1)\n",
    "\n",
    "scaleX = StandardScaler().fit_transform(X)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(xtrain, ytrain)\n",
    "ypred = rfc.predict(xtest)\n",
    "print(\"Accuracy: \", accuracy_score(ytest, ypred))\n",
    "\n",
    "imp_features = pd.DataFrame({'Features':X.columns, 'Importance':rfc.feature_importances_}).sort_values('Importance', ascending=False)\n",
    "print(imp_features)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(xtrain, ytrain)\n",
    "ypred_1 = dt.predict(xtest)\n",
    "print(\"Accuracy: \", accuracy_score(ytest, ypred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_lab_practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
